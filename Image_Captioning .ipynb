{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CS550 Project - Image Captioning"
      ],
      "metadata": {
        "id": "KwqwvmzdXEvq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-kzN3NZUtkk"
      },
      "outputs": [],
      "source": [
        "#Importing Necessary Libraries\n",
        "\n",
        "from os import listdir\n",
        "from pickle import dump\n",
        "import tensorflow as tf\n",
        "from zipfile import ZipFile\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check gpu connectivity\n",
        "\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "mn3EhcihYahH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1528c076-62d6-4a4b-f848-249f1cf1a6dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from each photo in the directory\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "from os import listdir\n",
        "\n",
        "def extract_features(image_directory):\n",
        "    base_model = VGG16(weights='imagenet')\n",
        "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)\n",
        "\n",
        "    extracted_features = dict()\n",
        "\n",
        "    for image_name in listdir(image_directory):\n",
        "        image_path = image_directory + '/' + image_name\n",
        "        image = load_img(image_path, target_size=(224, 224))\n",
        "        image_array = img_to_array(image)\n",
        "        image_array = image_array.reshape((1, image_array.shape[0], image_array.shape[1], image_array.shape[2]))\n",
        "        preprocessed_image = preprocess_input(image_array)\n",
        "\n",
        "        features = feature_extractor.predict(preprocessed_image, verbose=0)\n",
        "        image_id = image_name.split('.')[0]\n",
        "        extracted_features[image_id] = features\n",
        "\n",
        "    return extracted_features"
      ],
      "metadata": {
        "id": "45YsajAIXArz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"/content/drive/MyDrive/ML_Project/Images.zip\"\n",
        "with ZipFile(file_name,'r') as zip:\n",
        "  zip.extractall(\"/content/Flicker8k\")\n",
        "  print('Dataset Loaded')"
      ],
      "metadata": {
        "id": "2YsOuBv0aJKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c9f20a-efe6-451e-a487-d62838683468"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting features from the actual dataset\n",
        "\n",
        "directory = '/content/Flicker8k'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "Tw2W92uAXC5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9716e88-7ad7-42e1-932f-232862140a65"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 25s 0us/step\n",
            "Extracted Features: 8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the descriptions given the document\n",
        "\n",
        "import string\n",
        "\n",
        "def read_document(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        document = file.read()\n",
        "    return document\n",
        "\n",
        "def parse_descriptions(document):\n",
        "    descriptions_mapping = dict()\n",
        "    for line in document.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        if len(tokens) < 2:\n",
        "            continue\n",
        "\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        image_id = image_id.split('.')[0]\n",
        "        image_desc = ' '.join(image_desc)\n",
        "\n",
        "        if image_id not in descriptions_mapping:\n",
        "            descriptions_mapping[image_id] = list()\n",
        "        descriptions_mapping[image_id].append(image_desc)\n",
        "\n",
        "    return descriptions_mapping"
      ],
      "metadata": {
        "id": "M4Wi_JObeX0Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the descriptions\n",
        "\n",
        "import string\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "    punctuation_table = str.maketrans('', '', string.punctuation)\n",
        "    for image_id, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            description = desc_list[i]\n",
        "            words = description.split()\n",
        "            words = [word.lower() for word in words]\n",
        "            words = [word.translate(punctuation_table) for word in words]\n",
        "            words = [word for word in words if len(word) > 1]\n",
        "            words = [word for word in words if word.isalpha()]\n",
        "            desc_list[i] = ' '.join(words)\n",
        "\n",
        "def create_vocabulary(descriptions):\n",
        "    vocabulary = set()\n",
        "    for key in descriptions.keys():\n",
        "        [vocabulary.update(description.split()) for description in descriptions[key]]\n",
        "    return vocabulary\n",
        "\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()"
      ],
      "metadata": {
        "id": "BvAbGp83gN8w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using the aove defined functions\n",
        "\n",
        "filename = '/content/drive/MyDrive/ML_Project/captions.txt'\n",
        "doc = read_document(filename)\n",
        "descriptions = parse_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "\n",
        "clean_descriptions(descriptions)\n",
        "vocabulary = create_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIMhVBDpgQK_",
        "outputId": "d03c9932-8cdd-4e5e-a3df-e58191befdd1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: 8091 \n",
            "Vocabulary Size: 8680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#tokenizing the descriptions\n",
        "\n",
        "def load_image_ids(filename):\n",
        "    document = read_document(filename)\n",
        "    image_ids = set()\n",
        "    for line in document.split('\\n'):\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        identifier = line.split('.')[0]\n",
        "        image_ids.add(identifier)\n",
        "    return image_ids\n",
        "\n",
        "def load_descriptions(filename, image_ids):\n",
        "    document = read_document(filename)\n",
        "    descriptions = dict()\n",
        "    for line in document.split('\\n'):\n",
        "        tokens = line.split()\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        if image_id in image_ids:\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            descriptions[image_id].append(desc)\n",
        "    return descriptions\n",
        "\n",
        "def load_image_features(filename, image_ids):\n",
        "    all_features = load(open(filename, 'rb'))\n",
        "    features = {k: all_features[k] for k in image_ids}\n",
        "    return features\n",
        "\n",
        "def max_length(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)\n",
        "\n",
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "def create_description_tokenizer(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "TJbitBrRgURW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/ML_Project/Flickr_8k.trainImages.txt'\n",
        "train = load_image_ids(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "\n",
        "train_descriptions = load_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "\n",
        "max_len = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_len)\n",
        "\n",
        "train_features = load_image_features('features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "\n",
        "tokenizer = create_description_tokenizer(train_descriptions)\n",
        "dump(tokenizer, open('tokeniazer.pkl', 'wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "print(\"Traning set loaded\")\n",
        "\n",
        "filename = '/content/drive/MyDrive/ML_Project/Flickr_8k.testImages.txt'\n",
        "test = load_image_ids(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_descriptions('descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_image_features('features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "print(\"Test set loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozXxcEDBlxcl",
        "outputId": "28ee2731-b96d-4ece-8c01-0ab7c6294964"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Description Length: 33\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7507\n",
            "Traning set loaded\n",
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n",
            "Test set loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "import tensorflow\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from numpy import argmax\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from keras.layers import concatenate\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Aoby1U6Uo1Hb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the LSTM Model\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    for desc in desc_list:\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "        for i in range(1, len(seq)):\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "            X1.append(photo)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "\n",
        "    return array(X1), array(X2), array(y)\n",
        "\n",
        "\n",
        "def define_model(vocab_size, max_length):\n",
        "\n",
        "\tinputs1 = Input(shape=(1000,))\n",
        "\tfe1 = Dropout(0.5)(inputs1)\n",
        "\tfe2 = Dense(256, activation='relu')(fe1)\n",
        "\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "\tse2 = Dropout(0.5)(se1)\n",
        "\tse3 = LSTM(256)(se2)\n",
        "\n",
        "\tdecoder1 = add([fe2, se3])\n",
        "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
        "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "\t#print(model.summary())\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "zXjp_ukmp61a"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(descriptions, photos, tokenizer, max_length):\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
        "\t\t\tyield [[in_img, in_seq], out_word]"
      ],
      "metadata": {
        "id": "6SF2vGSS6jOr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "\n",
        "model = define_model(vocab_size, max_len)\n",
        "epochs = 20\n",
        "steps = len(train_descriptions)\n",
        "print(\"length is :\" ,steps)\n",
        "for i in range(epochs):\n",
        "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_len)\n",
        "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\tmodel.save('model_' + str(i) + '.h5')"
      ],
      "metadata": {
        "id": "7eVoe_fX6orx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding starts and end keywords\n",
        "\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "\tin_text = 'startseq'\n",
        "\n",
        "\tfor i in range(max_length):\n",
        "\n",
        "\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n",
        "\t\tyhat = model.predict([photo,sequence], verbose=0)\n",
        "\t\tyhat = argmax(yhat)\n",
        "\t\tword = word_for_id(yhat, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\tin_text += ' ' + word\n",
        "\t\tif word == 'endseq':\n",
        "\t\t\tbreak\n",
        "\treturn in_text"
      ],
      "metadata": {
        "id": "7L3EObQgrxXT"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "_8qOnBIgsKSb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'model_0.h5'\n",
        "model = load_model(filename)\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACuoQpY4sO0i",
        "outputId": "6ed4208e-efa6-4e23-d12d-036e7d6714a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.566379\n",
            "BLEU-2: 0.295063\n",
            "BLEU-3: 0.194548\n",
            "BLEU-4: 0.081796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the model on a image\n",
        "\n",
        "tokenizer = load(open('tokeniazer.pkl', 'rb'))\n",
        "\n",
        "max_length = 33\n",
        "model = load_model('model_0.h5')\n",
        "\n",
        "def extract_feature(filename):\n",
        "\tmodel1 = VGG16()\n",
        "\tmodel1.layers.pop()\n",
        "\tmodel1 = Model(inputs=model1.inputs, outputs=model1.layers[-1].output)\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\timage = img_to_array(image)\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\timage = preprocess_input(image)\n",
        "\tfeature = model1.predict(image, verbose=0)\n",
        "\treturn feature\n",
        "\n",
        "photo = extract_feature('image.jpg')\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "\n",
        "query = description\n",
        "stopwords = ['startseq','endseq']\n",
        "querywords = query.split()\n",
        "resultwords  = [word for word in querywords if word.lower() not in stopwords]\n",
        "result = ' '.join(resultwords)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "b9dhG_s3tR7E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}